---
title: "Data preparation for modelling"
order: 1
---

## Introduction

The aim of the modelling process is to predict "next year's" target variable from information that has occurred in previous years. 
In the modelling process, the "target variable" is the variable, or performance metric, that the modelling process is attempting to predict. Each performance metric will have a different model that creates the best predictions, but the process to identify the best model will be the same.

The models are all evaluated using the R^2^ metric (coefficient of determination), which is the proportion of the variance of the target variable that is explained by the predictor variable. 

## Data definitons

Each metric (performance, demand and capacity) has been collated with a numerator, a denominator and a calculated value, which is either a proportion or a rate. 

By its nature, a proportion is always between 0 and 1. All other values can only take a positive number. 

For modelling, the data used could be either the "value" (e.g., the calculated proportion or rate) or the numerator. The performance of the modelling process could be dependent on the modelling technique used.

## Data structure

For modelling, the data are structured as a record for each Integrated Care System (ICS) for each year.
The fields will contain the target variable (the metric that the model is trying to predict) alongside lagged values of the predictor variables. The number of years to lag the predictor variables needs to be determined and could potentially be model specific.

## Splitting dataset

Data need to be split before training model. Some considerations are:

* Should the data be ordered prior to splitting (eg, should models be trained on earlier years, and tested on later years) - on occasion there won't be very long time series
  + try mixing the data up before splitting
* Train, validation and test set? Is there enough data to do this?

## Missing data

* How to accommodate independent providers in the metrics
* The input data has missing values. The data needs more investigation as to why it is missing (it could be really missing, eg, data not submitted, or it could be a zero, eg, a zero numerator for that metric). 

  - Some timeseries data for Trusts (eg beds available) stop being reported and it is difficult to understand why (eg RT5 stops being reported in Q1 2015 even though the trust still exists)
  - Some "Independent Sector Healthcare providers" stop reporting Sickness data mid-timeseries

* Need to investigate different imputation methods for different features

## Modelling

* Try out different ML methods?
* Should demand and capacity variables be modelled separately (like [Charlotte's ED modelling](https://charlottejames.github.io/ed-forecast/index.html))

## Other questions

* What to do with IMD? It is published by LSOA for a single year. Should the information for each ICS be applied for each year? Should it relate to the population variables? How else to integrate deprivation into the analysis?

## Next steps

* try running other performance metrics through the modelling process to see if others come out better
* improve hyperparameter tuning for RF
* check whether missing data are missing or zero (values could be zero but come through as missing because the numerator is zero)
* Stepwise modelling for linear/logistic (step aic - more popular at the moment; step bic)
* How to keep the predictions between 0 and 1? Does it mean the model should be a logistic regression?
* collect more data
* feature engineering
  + combine total beds with population (or population in need, eg, population over 65)
* demonstrate proof of concept to stakeholders
  + identify other metrics that would be useful to predict
* demonstrate scenario modelling

### Notes

Parametric modelling should have more thought on the input features, whereas ML potentially can throw them all in and see what happens