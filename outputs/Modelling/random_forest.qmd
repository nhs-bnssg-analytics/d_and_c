---
title: "Random Forest"
---

## Setup

Load in the libraries and useful functions.

```{r}
#| label: libraries
#| message: false
source(here::here("R/00_libraries.R"))
source(here::here("R/01_utils.R"))

```

Load in the pre-prepared data.

```{r}
#| label: data
metrics <- read.csv(here::here("data/configuration-table.csv"),
                    encoding = "latin1") |> 
  filter(!(status %in% c("incorrect geography", "remove"))) |> 
  select(metric)

dc_data <- list.files(here::here("data"), full.names = TRUE) |> 
  (\(x) x[!grepl("configuration-table|modelling_data", x)])() |> 
  purrr::map_dfr(
    read.csv
  ) |> 
  inner_join(
    metrics,
    by = join_by(metric)
  ) |> 
  filter(
    grepl("annual", frequency),
    grepl("^Q", org)
    ) |> 
  select(
    "metric",
    "year", 
    "org", 
    "value"
  ) |> 
  pivot_wider(
    names_from = metric,
    values_from = value
  )
```

## Preprocessing

Create dataframe with target and predictor variables.

```{r}
#| label: pre-processing
target_variable <- "Proportion of completed pathways greater than 18 weeks from referral (admitted)"

dc_data <- dc_data |> 
  select(
    all_of(c("org", "year", target_variable)),
    matches("^ESR|^Workforce|^Bed|age band|Year 6|GPPS")
  ) |> 
  dplyr::filter(
    # retain all rows where target variable is not na
    # retain all rows where we have population age group information
    if_all(
      all_of(
        c(target_variable, "Proportion of population in age band (80-89)")), ~ !is.na(.))
  )

# rows that contain nas
rowSums(is.na(dc_data))

# columns that contain nas
colSums(is.na(dc_data)) |> 
  tibble::enframe()
```

Add lag variables.

```{r}
#| label: lag-variables

dc_data <- dc_data |> 
  mutate(
    across(
      matches("^ESR|^Workforce|^Bed|age band|Year 6|GPPS"),
      .names = "lag1_{.col}",
      .fns = lag
    ),
    .by = org
  ) |> 
  arrange(year, org)
```


## {tidymodels}

* Split the data
* Select the modelling method (random forest from the ranger package)
* Impute missing data (median)

```{r}
#| label: splitting-data
set.seed(321)

# split dataset into train, validation and test
splits <- rsample::initial_validation_time_split(
  data = dc_data#,
  # lag = 1
)

data_train <- rsample::training(splits)
data_validation <- rsample::validation(splits)
data_test <- rsample::testing(splits)
data_train_validation <- bind_rows(
  data_train,
  data_validation
)

# check when train, validation and test data start and finish
lapply(
  list(data_train, data_validation, data_test),
  function(x) range(x$year)
)

data_validation_set <- validation_set(splits)

# how many cores on the machine so we can parallelise
cores <- parallel::detectCores()

# set model
rf_mod <- 
  rand_forest(
    mtry = tune(), 
    min_n = tune(), 
    trees = 1000
  ) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("regression")

# create recipes
# interaction variables (year and variables?)

# impute missings?
# variables with missing data
missing_data <- names(dc_data)[colSums(is.na(dc_data)) > 0]

rf_recipe <-
  recipe(`Proportion of completed pathways greater than 18 weeks from referral (admitted)` ~ ., 
         data = data_train_validation) %>%
  step_impute_median(all_of(missing_data))# %>%
  # step_lag(matches("^ESR|^Workforce|^Bed|age band|Year 6|GPPS"),
  #          lag = 1:2)

```

Set up {tidymodels} workflow:

* Add model to workflow
* Add recipe to workflow


```{r}
#| label: workflow


# add recipe to workflow
rf_workflow <- 
  workflow() |> 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)

```

Tune the hyperparameters.

```{r}
#| label: tune-hyperparameters


# design the tuning of the hyperparameters
rf_res <- 
  rf_workflow %>% 
  tune_grid(data_validation_set,
            grid = 50,
            control = control_grid(save_pred = TRUE))

# show the best parameters
rf_res %>% 
  show_best(metric = "rmse")


autoplot(rf_res)
```


```{r}
#| label: predict-test-data
# select the best parameters
rf_best <- 
  rf_res %>% 
  select_best(metric = "rmse")


# the last model
last_rf_mod <- 
  rand_forest(mtry = rf_best$mtry, min_n = rf_best$min_n, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("regression")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit %>% 
  collect_metrics()
```


## Variable importance

```{r}
#| label: variable-importance
last_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip::vip(num_features = 20)

```

